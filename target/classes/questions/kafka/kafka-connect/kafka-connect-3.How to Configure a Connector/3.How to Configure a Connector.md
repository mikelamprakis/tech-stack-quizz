## Question 1

```markdown
Which field in a Kafka Connector JSON config specifies the class to be used for the connector?
```

**Options**

```markdown
- A. connector.name
- B. connector.type
- C. connector.class
- D. class.name
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Not a valid field
- B. Incorrect naming
- C. ✅ Correct – This defines which Java class Kafka Connect should use
- D. Not a recognized Kafka Connect field
```

</details>

---

## Question 2

```markdown
What is the purpose of the `auto.create` field in a JDBC Sink connector?
```

**Options**

```markdown
- A. Automatically starts Kafka Connect when the cluster boots
- B. Creates Kafka topics if they don’t exist
- C. Automatically creates database tables if they don’t exist
- D. Automatically evolves Kafka schema
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Connect starts independently
- B. Topic creation is handled by Kafka, not the connector
- C. ✅ Correct – `auto.create: true` allows table creation
- D. Schema evolution is handled separately via `auto.evolve`
```

</details>

---

## Question 3

```markdown
What is the function of the `flush.size` parameter in the JDBC Sink connector configuration?
```

**Options**

```markdown
- A. Number of records to buffer in Kafka
- B. Size in bytes to flush logs
- C. Number of records after which to flush data to the database
- D. Time interval to commit offsets
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Buffering in Kafka is not related
- B. Not related to log flushing
- C. ✅ Correct – `flush.size` controls when to write batched data to the sink
- D. Offset commits are separate from this behavior
```

</details>

---

## Question 4

```markdown
Which of the following connectors is designed to stream data from Kafka to Amazon S3?
```

**Options**

```markdown
- A. JDBC Sink Connector
- B. S3 Sink Connector
- C. SFTP Source Connector
- D. JMS Source Connector
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Sends to relational databases
- B. ✅ Correct – S3 Sink pushes data to Amazon S3
- C. Reads data from SFTP into Kafka
- D. Reads messages from JMS into Kafka
```

</details>

---

## Question 5

```markdown
Which of the following tools can be used to submit a connector configuration to Kafka Connect?
```

**Options**

```markdown
- A. Kafka Streams API
- B. Kafka Connect UI
- C. Kafka REST Proxy
- D. curl with Kafka Connect REST API
```

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**

```markdown
- A. Kafka Streams is for processing
- B. There is no official Kafka Connect UI
- C. REST Proxy is for producing/consuming, not connector management
- D. ✅ Correct – You can POST the config using curl to the Kafka Connect REST API
```

</details>

---

## Question 6

```markdown
What is the correct `insert.mode` value to configure insert-only behavior in JDBC Sink Connector?
```

**Options**

```markdown
- A. upsert
- B. append
- C. insert
- D. write
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Upsert combines insert + update logic
- B. Not a valid insert.mode
- C. ✅ Correct – `insert` ensures new rows are added
- D. Not a supported option
```

</details>

---

## Question 7

```markdown
Which configuration specifies the Kafka topics used by a connector?
```

**Options**

```markdown
- A. kafka.source
- B. input.topics
- C. topics
- D. kafka.topics
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Invalid field
- B. Not a recognized configuration
- C. ✅ Correct – `topics` defines the Kafka topics used
- D. Not an official config key
```

</details>

---

## Question 8

```markdown
Which of the following is NOT a valid troubleshooting step if a connector fails?
```

**Options**

```markdown
- A. Check connector logs
- B. Verify the Kafka topic exists
- C. Restart Kafka Connect service with `kafka-topics.sh`
- D. Use REST API to check connector status
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. ✅ Recommended for error insights
- B. ✅ Important to verify source/sink topics
- C. ❌ Incorrect – `kafka-topics.sh` is unrelated to Kafka Connect service
- D. ✅ REST API helps monitor connector state
```

</details>

---

## Question 9

```markdown
What does the `auto.evolve` setting enable in JDBC Sink connectors?
```

**Options**

```markdown
- A. Automatic creation of Kafka topics
- B. Schema evolution in destination database tables
- C. Updating worker configurations on restart
- D. Auto-scaling of Kafka Connect workers
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Kafka topic creation is separate
- B. ✅ Correct – `auto.evolve: true` lets schemas evolve with new fields
- C. Not related to worker behavior
- D. Scaling is manual or orchestrated separately
```

</details>

---

## Question 10

```markdown
What is the key difference between connector config and worker config in Kafka Connect?
```

**Options**

```markdown
- A. Connector config defines logging levels
- B. Worker config specifies external DBs to connect to
- C. Connector config controls connector behavior, worker config controls the execution environment
- D. Worker config defines Kafka topic schemas
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Logging is part of worker setup, but not connector config
- B. External system connection is defined in connector config
- C. ✅ Correct – Connector config = task behavior, Worker config = Kafka Connect environment
- D. Topic schemas are not defined in worker config
```

</details>

---

## Question 11

```markdown  
When using plain JSON data with Connect, you see the following error message:  
org.apache.kafka.connect.errors.DataException: JsonDeserializer with schemas.enable requires "schema" and "payload"  
fields and may not contain additional fields. How will you fix the error?  
```  

**Options**
```markdown  
- A. Set key.converter.schema.enable = true and value.converter.schemas.enable to false  
- B. Set value.converter.schemas.enable to Avro converter  
- C. Set value.converter.schemas.enable to Json converter  
- D. Set key.converter.schemas.enable to false  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
The error occurs when schema validation is enabled for plain JSON data.  

- A. Correct. Disabling schema validation for values fixes this error  
- B. Switching to Avro doesn't solve the JSON schema requirement  
- C. This maintains the same schema validation issue  
- D. Only addresses key conversion, not the value error  
```  

</details>  

---  

## Question 12

```markdown  
You are using JDBC source connector to copy data from 3 tables to three Kafka topics.  
There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers.  
How many tasks are launched?  
```  

**Options**
```markdown  
- A. 1  
- B. 3  
- C. 6  
- D. 2  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
The number of tasks is determined by max.tasks configuration.  

- A. Would underutilize the configured capacity  
- B. Exceeds the max.tasks limit  
- C. Incorrectly multiplies tables and workers  
- D. Correct. The connector creates tasks up to max.tasks limit  
```  

</details>  

---

## Question 13

```markdown  
Where are the Kafka Connect connector configurations stored?  
```  

**Options**
```markdown  
- A. In a separate config file on each Kafka Connect worker  
- B. In the Kafka broker's config directory  
- C. In Zookeeper under the `/kafka-connect` znode  
- D. In a special Kafka topic named `connect-configs`  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
Kafka Connect uses internal topics for configuration storage.  

- A. Configs are not worker-local  
- B. Brokers don't store connector configs  
- C. Zookeeper is not used for this purpose  
- D. Correct. Configs are stored in Kafka topics  
```  

</details>  

---  

## Question 14

```markdown  
You are using a JDBC source connector to copy data from a database table to a Kafka topic. The table has 5 columns. How many tasks will be created by the connector?  
```  

**Options**
```markdown  
- A. 1  
- B. 5  
- C. It depends on the `max.tasks` configuration of the connector  
- D. It depends on the number of partitions in the Kafka topic  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
JDBC connectors create one task per table by default.  

- A. Correct. Number of columns doesn't affect task count  
- B. Tasks aren't created per column  
- C. `max.tasks` doesn't override per-table behavior  
- D. Partition count doesn't affect source task count  
```  

</details>  

---  

## Question 15

```markdown  
What happens if the `max.tasks` configuration is set to a value less than the number of tables being copied by a JDBC source connector?  
```  

**Options**
```markdown  
- A. The connector will create one task per table, ignoring the `max.tasks` setting  
- B. The connector will create tasks up to the `max.tasks` limit, potentially leaving some tables without dedicated tasks  
- C. The connector will distribute the tables evenly among the available tasks  
- D. The connector will fail with an error due to the insufficient number of tasks  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
The connector intelligently shares tables across tasks.  

- A. `max.tasks` is always respected  
- B. No tables are left unprocessed  
- C. Correct. Tables are load-balanced across tasks  
- D. The connector handles this scenario gracefully  
```  

</details>  

---  