## Question 1

```markdown
Where are Single Message Transforms (SMTs) configured in Kafka Connect?
```

**Options**

```markdown
- A. In the Kafka broker
- B. In the Kafka Connect worker configuration
- C. In the connector configuration
- D. In ZooKeeper
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Kafka brokers are not involved in SMTs.
- B. Worker configs define environment, not transformations.
- C. ✅ Correct – SMTs are applied via the connector config.
- D. ZooKeeper is not used for SMTs.
```

</details>

---

## Question 2

```markdown
What is the purpose of the `InsertField` SMT in Kafka Connect?
```

**Options**

```markdown
- A. To promote a field to the record key
- B. To dynamically route topics
- C. To add a timestamp or metadata to records
- D. To remove fields from a record
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. That's done by `ValueToKey`.
- B. That’s the job of `RegexRouter`.
- C. ✅ Correct – `InsertField` adds fields like timestamps or hostnames.
- D. That’s done using `ReplaceField`.
```

</details>

---

## Question 3

```markdown
Which SMT allows dynamic renaming of Kafka topics?
```

**Options**

```markdown
- A. InsertField
- B. RegexRouter
- C. ValueToKey
- D. ReplaceField
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Adds fields but doesn’t modify topic names.
- B. ✅ Correct – `RegexRouter` rewrites topic names using regex.
- C. Promotes fields to record key.
- D. Removes or retains fields.
```

</details>

---

## Question 4

```markdown
Which SMT would you use to promote a `user_id` field to be the Kafka record key?
```

**Options**

```markdown
- A. ReplaceField
- B. InsertField
- C. RegexRouter
- D. ValueToKey
```

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**

```markdown
- A. Used for blacklisting or whitelisting fields.
- B. Adds fields like timestamps.
- C. Routes topics based on patterns.
- D. ✅ Correct – `ValueToKey` promotes a value field to key.
```

</details>

---

## Question 5

```markdown
In the Elasticsearch logging use case, which SMT is used to remove sensitive fields?
```

**Options**

```markdown
- A. InsertField
- B. ReplaceField
- C. RegexRouter
- D. ValueToKey
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Adds metadata like timestamps.
- B. ✅ Correct – `ReplaceField` with `blacklist` removes sensitive fields.
- C. Changes topic names dynamically.
- D. Moves fields into the record key.
```

</details>

---

## Question 6

```markdown
What does the SMT configuration `transforms.RouteTopic.regex` define?
```

**Options**

```markdown
- A. The default topic to write to
- B. The regex pattern used to rewrite the topic name
- C. The key field to route the record
- D. The header transformation behavior
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Topic is not statically defined this way.
- B. ✅ Correct – it defines the pattern to match existing topic names.
- C. Topic routing is not based on key field in RegexRouter.
- D. Headers are not involved in this transformation.
```

</details>

---

## Question 7

```markdown
Why might someone use SMTs instead of preprocessing data externally?
```

**Options**

```markdown
- A. Because SMTs are more secure
- B. Because SMTs offer custom analytics
- C. Because SMTs handle large batch ETL operations
- D. Because SMTs enable lightweight, inline transformations
```

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**

```markdown
- A. SMTs are not focused on security.
- B. They don’t perform analytics.
- C. SMTs are for per-record transformations, not batch ETL.
- D. ✅ Correct – SMTs are meant for simple, inline record-level tweaks.
```

</details>

---

## Question 8

```markdown
Which field is used in the `InsertField` SMT example to add the processing timestamp?
```

**Options**

```markdown
- A. created_at
- B. timestamp
- C. processed_at
- D. updated_time
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Not used in this example.
- B. Too generic and not specified in the config.
- C. ✅ Correct – `processed_at` is the field added.
- D. Not mentioned in the file.
```

</details>

---

## Question 9

```markdown
In the Elasticsearch example, what pattern is used to rename topics?
```

**Options**

```markdown
- A. logs.* to elasticsearch.*
- B. logs-(.*) to elasticsearch-logs-$1
- C. .*logs to logs_es
- D. logs_ to es_logs_
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Incorrect syntax and structure.
- B. ✅ Correct – this pattern matches `logs-(.*)` and renames to `elasticsearch-logs-$1`.
- C. Incorrect structure.
- D. Not used in the file.
```

</details>

---

## Question 10

```markdown
What is the main difference between connector config and worker config in the context of SMTs?
```

**Options**

```markdown
- A. SMTs can only be configured at the worker level
- B. Worker config defines how SMTs transform records
- C. Connector config handles SMTs; worker config handles the environment
- D. Worker config includes source/sink settings
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. False – SMTs are defined at the connector level.
- B. Worker config doesn’t transform data.
- C. ✅ Correct – SMTs are part of connector config; worker config is infrastructure-related.
- D. Source/sink details go in the connector config.
```

</details>

---

## Question 11

```markdown  
A retail chain wants to integrate sales data from their Point of Sale (POS) systems across multiple stores into Kafka for real-time analysis and inventory management. Each store's POS system dumps sales records into a local SQL database. The integration needs to account for network bandwidth limitations. Which strategy optimally addresses these requirements?  
```  

**Options**
```markdown  
- A. Deploy Kafka Connect with the JDBC Source Connector at each store to ingest sales data into Kafka, using Single Message Transforms (SMTs) to filter and reduce the size of the data on the fly. Aggregate this data centrally using a Kafka Streams application for inventory analysis  
- B. Utilize log-based Change Data Capture (CDC) connectors to monitor changes in each store's SQL database, streaming only new or changed sales records into Kafka. This minimizes network usage and enables real-time central processing with Kafka Streams  
- C. Implement custom Kafka Producers within the POS systems to directly publish sales data to Kafka, compressing messages to mitigate network bandwidth issues. Use Kafka Streams for processing and inventory management centrally  
- D. Set up a central database to aggregate sales data from all stores nightly. Use the JDBC Sink Connector to transfer this aggregated data into Kafka for next-day inventory analysis, relying on batch processing rather than real-time analysis  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
CDC provides efficient, incremental data transfer.  

- A. Full data transfer may strain bandwidth  
- B. Correct. Only transfers changes, minimizing bandwidth  
- C. Custom solution increases maintenance  
- D. Batch processing doesn't meet real-time needs  
```  

</details>  
