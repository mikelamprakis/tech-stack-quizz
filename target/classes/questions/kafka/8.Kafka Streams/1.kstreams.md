## Question 1

```markdown
Which of the following best describes a KStream in Kafka Streams?
```

**Options**

```markdown
- A. A continuously updating table of data
- B. A snapshot of the current state of a system
- C. An unbounded, append-only stream of records
- D. A set of aggregated records
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
A KStream represents an unbounded stream of immutable records (events).

- A. That's a KTable.
- B. A snapshot aligns more with KTable.
- C. Correct. KStreams are append-only, event-by-event.
- D. Aggregated records are results of stateful processing.
```

</details>

---

## Question 2

```markdown
When you perform a `mapValues` on a KStream, what happens?
```

**Options**

```markdown
- A. The key is transformed
- B. The value is transformed
- C. The key and value are both transformed
- D. Nothing happens
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
`mapValues` transforms only the value while keeping the key unchanged.

- A. Only values are affected.
- B. Correct.
- C. For transforming both, use `map`.
- D. It does apply the transformation function.
```

</details>

---

## Question 3

```markdown
Which operation can convert a KStream into a KTable?
```

**Options**

```markdown
- A. `flatMap`
- B. `groupByKey().reduce()`
- C. `to()`
- D. `print()`
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Reducing or aggregating a grouped KStream results in a KTable.

- A. `flatMap` is for one-to-many mapping.
- B. Correct. It gives you a materialized KTable.
- C. Writes the stream to a topic, doesn’t convert it.
- D. Only prints the stream.
```

</details>

---

## Question 4

```markdown
Which method would you use to output a KStream to a topic?
```

**Options**

```markdown
- A. `join()`
- B. `to()`
- C. `groupBy()`
- D. `aggregate()`
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
`to()` writes records from the KStream to a Kafka topic.

- A. `join()` is for combining data.
- B. Correct.
- C. Used for grouping, not output.
- D. Used for aggregation, not writing out.
```

</details>

---

## Question 5

```markdown
Which Kafka Streams class represents a stream of key-value records?
```

**Options**

```markdown
- A. `KStream`
- B. `KTable`
- C. `StreamsBuilder`
- D. `KafkaConsumer`
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
KStream is the fundamental abstraction for event streams.

- A. Correct.
- B. Represents tables, not streams.
- C. Used to build topologies.
- D. From the consumer API, not Streams API.
```

</details>

---

## Question 6

```markdown
What is the result of applying `filter()` on a KStream?
```

**Options**

```markdown
- A. A new stream with all records unchanged
- B. A stream of aggregated results
- C. A stream with only the records that match the predicate
- D. A stream with grouped records
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
`filter()` removes records that don't match the condition.

- A. Incorrect, the stream is modified.
- B. Not an aggregation.
- C. Correct.
- D. Use `groupBy` to group.
```

</details>

---

## Question 7

```markdown
How does a KStream handle duplicate keys?
```

**Options**

```markdown
- A. It updates the old value with the new one
- B. It keeps all records even with duplicate keys
- C. It throws an error
- D. It deletes the old record
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
KStream treats every record as an independent event.

- A. That's KTable behavior.
- B. Correct.
- C. No error occurs.
- D. KStreams are append-only.
```

</details>

---

## Question 8

```markdown
What does `flatMap()` do in a KStream?
```

**Options**

```markdown
- A. Filters the stream
- B. Maps one input to multiple outputs
- C. Aggregates values
- D. Outputs the same record multiple times
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
`flatMap` can emit multiple records for each input.

- A. Use `filter()` for that.
- B. Correct.
- C. Aggregation needs grouping first.
- D. It emits based on logic, not repetition.
```

</details>

---

## Question 9

```markdown
Which operator in KStream is used to combine two streams together based on key?
```

**Options**

```markdown
- A. `join()`
- B. `groupByKey()`
- C. `filter()`
- D. `selectKey()`
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
`join()` is for combining records with matching keys.

- A. Correct.
- B. Used for aggregation.
- C. Filters records, not joins.
- D. Used to change key, not combine streams.
```

</details>

---

## Question 10

```markdown
Which of the following operations is stateless on a KStream?
```

**Options**

```markdown
- A. `filter()`
- B. `reduce()`
- C. `aggregate()`
- D. `count()`
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
`filter()` processes each record individually.

- A. Correct.
- B. Requires state.
- C. Stateful.
- D. Stateful.
```

</details>

## Question 11

```markdown
In a Kafka Streams app monitoring ERROR logs, which is the best configuration for the source topic?

StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> logs = builder.stream("application-logs");
KStream<String, String> errorLogs = logs.filter((key, value) -> value.contains("ERROR"));
errorLogs.to("error-logs-topic");
builder.build();
```

**Options**

```markdown
- A. retention.bytes = -1
- B. cleanup.policy = compact
- C. min.insync.replicas = 2
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
retention.bytes = -1 means no size-based deletion, allowing the app to process all logs.

- A. Correct. Prevents logs from being deleted due to size.
- B. Compaction is for deduplication, not raw logs.
- C. Improves durability, but not performance-critical here.
```

</details>

---

## Question 12

```markdown
Is Kafka Streams DSL ANSI SQL compliant?
```

**Options**

```markdown
- A. Yes
- B. No
- C. Partially
- D. Depends on version
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Kafka Streams DSL is a fluent Java API and not based on ANSI SQL.

- A. Wrong. It's not SQL-based.
- B. Correct. It’s a Java DSL, not SQL.
- C. Misleading — no partial compliance.
- D. Not version-dependent.
```

</details>

---

## Question 13

```markdown
What is the primary language used for writing Kafka Streams applications?
```

**Options**

```markdown
- A. Python
- B. Java
- C. Scala
- D. SQL
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Kafka Streams is a Java library, so its primary language is Java.

- A. Wrong. No native support.
- B. Correct. Official Kafka Streams API is Java-based.
- C. Possible via JVM, but not primary.
- D. SQL is for ksqlDB, not Streams DSL.
```

</details>

---

## Question 14

```markdown
What is the main advantage of using Kafka Streams DSL over the Processor API for stream processing?
```

**Options**

```markdown
- A. Kafka Streams DSL provides better performance compared to the Processor API
- B. Kafka Streams DSL offers a higher-level, declarative approach to defining stream processing logic
- C. Kafka Streams DSL supports stateful operations, while the Processor API is limited to stateless operations
- D. Kafka Streams DSL allows for easier integration with external systems compared to the Processor API
```

<details><summary>Response:</summary> 

**Answer:** B

**Explanation:**

```markdown
The main advantage of using Kafka Streams DSL (Domain-Specific Language) over the Processor API for stream processing is that it offers a higher-level, declarative approach to defining stream processing logic.

Kafka Streams DSL provides a set of high-level operations and constructs that allow developers to express stream processing logic in a more concise and expressive manner. With the DSL, you can chain together operations like `map`, `filter`, `groupBy`, `aggregate`, and `join` to define the desired stream processing topology. The DSL abstracts away low-level details and provides a more intuitive and readable way to define the processing logic.

On the other hand, the Processor API is a lower-level API that provides more fine-grained control over the stream processing topology. With the Processor API, you need to define individual processor nodes and connect them manually to create the desired processing flow. While this provides more flexibility, it requires more code and can be more complex to implement and maintain compared to the DSL.

Both Kafka Streams DSL and the Processor API offer similar performance characteristics and support stateful operations. They also provide integration capabilities with external systems. The choice between the two depends on the specific requirements of the application and the level of control and customization needed.


```

</details>


## Question 15

```markdown
In an IoT application, you have a stream of sensor readings that need to be processed in real-time. Which of the following is the most suitable way to model this data in Kafka Streams?
```

**Options**

```markdown
- A. As a KTable, with each sensor reading as a key-value pair
- B. As a KStream, with each sensor reading as a record
- C. As a GlobalKTable, with each sensor reading as a key-value pair
- D. As a windowed KTable, with each sensor reading as a key-value pair
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Sensor readings are continuous events and best modeled as a KStream for real-time processing.

- A. KTable is for key-based state updates, not ideal for raw sensor events.
- B. Correct. Each reading is a separate event, perfect for KStream.
- C. GlobalKTable is for reference/static data and isn't scalable for high-volume input.
- D. Windowed KTable may be used for aggregations but not for raw, real-time data.
```

</details>

---

## Question 16

```markdown
You are building a real-time analytics application that tracks user behavior on a website. Which of the following data is most appropriate to be modeled as a KStream in Kafka Streams?
```

**Options**

```markdown
- A. User demographic information
- B. User navigation events
- C. User purchase history
- D. User authentication data
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Navigation events represent real-time actions that are ideal for processing via a KStream.

- A. Demographics are relatively static and suited for KTable.
- B. Correct. Navigation events are high-volume, real-time, and event-driven.
- C. Purchase history can be modeled as a stream or table depending on context.
- D. Auth data is more static or session-based, not ideal for a KStream unless capturing auth events.
```

</details>

---

Let me know if you'd like these exported to a file or converted to another format (e.g., Quizlet, JSON, CSV, etc.).

## Question 17

```markdown
To transform data from a Kafka topic to another one, I should use:
```

**Options**

```markdown
- A. Consumer + Producer
- B. Kafka Connect Source
- C. Kafka Connect Sink
- D. Kafka Streams
```

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**

```markdown
Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka topics. Here’s the breakdown:

- A. Consumer + Producer – This works but lacks declarative streaming logic and state handling.
- B. Kafka Connect Source – Used for ingesting data into Kafka, not transforming it.
- C. Kafka Connect Sink – Used for exporting data out of Kafka.
- D. Kafka Streams – The correct tool for transforming data from one topic to another.
```

</details>

---

