## Question 1

```markdown
What is the primary role of Kafka Connect in the Kafka ecosystem?
```

**Options**

```markdown
- A. To manage Kafka clusters
- B. To write Kafka producer/consumer logic manually
- C. To stream data between Kafka and external systems
- D. To monitor Kafka throughput
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
Kafka Connect is designed to stream data between Kafka and external systems like databases, cloud storage, or search engines.

- A. Incorrect – That’s the job of tools like Kafka Manager or Cruise Control
- B. Incorrect – Kafka Connect avoids custom code
- C. Correct – It moves data into and out of Kafka
- D. Incorrect – Monitoring is a separate concern
```

</details>

---

## Question 2

```markdown
Which of the following are types of Kafka Connect connectors?
```

**Options**

```markdown
- A. Source and Sink
- B. Producer and Consumer
- C. Forwarder and Backwarder
- D. Publisher and Subscriber
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
Kafka Connect defines connectors as either source (ingest into Kafka) or sink (push out of Kafka).

- A. Correct – Source and sink are the two types
- B. Incorrect – Producer/Consumer is Kafka core, not Connect terminology
- C. Incorrect – Not valid connector types
- D. Incorrect – More relevant to messaging systems like MQTT
```

</details>

---

## Question 3

```markdown
What feature allows Kafka Connect to transform messages without writing a custom processor?
```

**Options**

```markdown
- A. Kafka Streams
- B. Partition Rebalancer
- C. SMT (Single Message Transforms)
- D. Schema Registry Hooks
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
SMTs allow lightweight message transformations during data flow.

- A. Incorrect – Kafka Streams is a separate processing API
- B. Incorrect – That’s used for scaling and balancing workers
- C. Correct – SMTs are built-in transforms in Kafka Connect
- D. Incorrect – Schema Registry helps validate formats, not transform messages
```

</details>

---

## Question 4

```markdown
Which mode is best suited for production environments with fault tolerance?
```

**Options**

```markdown
- A. Standalone mode
- B. Distributed mode
- C. Debug mode
- D. Clustered manual mode
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Distributed mode allows Kafka Connect to scale with multiple workers and provides fault tolerance.

- A. Incorrect – Standalone is used for local dev or tests
- B. Correct – Distributed is used in production
- C. Incorrect – No such mode exists
- D. Incorrect – Not a valid Kafka Connect mode
```

</details>

---

## Question 5

```markdown
Which of the following is a valid real-world use case for Kafka Connect?
```

**Options**

```markdown
- A. Running Kafka Streams in batch mode
- B. Consuming data from a Kafka topic in a GUI
- C. Loading data from MySQL into Kafka using a connector
- D. Managing ACLs for Kafka topics
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
Kafka Connect supports MySQL ingestion through a JDBC Source Connector.

- A. Incorrect – Kafka Streams is separate
- B. Incorrect – That’s unrelated to Kafka Connect
- C. Correct – A common and supported use case
- D. Incorrect – ACLs are managed at the broker level
```

</details>

---

## Question 6

```markdown
What does Kafka Connect automatically handle when configured properly?
```

**Options**

```markdown
- A. Auto-sharding Kafka brokers
- B. Schema validation and retry logic
- C. UI-based message editing
- D. Kafka topic deletion
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Kafka Connect manages schema validation (with Schema Registry) and retries failed messages.

- A. Incorrect – Broker-level behavior, not Kafka Connect
- B. Correct – Kafka Connect includes built-in support for these
- C. Incorrect – No GUI message editing
- D. Incorrect – Not responsible for topic deletion
```

</details>

---

## Question 7

```markdown
Which configuration mode allows quick tests using local files and a single process?
```

**Options**

```markdown
- A. Distributed mode
- B. Cluster mode
- C. Standalone mode
- D. Embedded mode
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
Standalone mode is used for quick, local testing without needing Kafka coordination.

- A. Incorrect – That’s for production
- B. Incorrect – Not an official term
- C. Correct – Standalone is used for dev/test
- D. Incorrect – Not an available mode in Kafka Connect
```

</details>

---

## Question 8

```markdown
Which of the following is an example of a Sink connector?
```

**Options**

```markdown
- A. MQTT Source Connector
- B. JDBC Sink Connector
- C. File Source Connector
- D. JDBC Source Connector
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
A Sink Connector pushes data from Kafka to an external system. JDBC Sink writes to databases.

- A. Incorrect – That’s a Source Connector
- B. Correct – JDBC Sink writes to a database
- C. Incorrect – Also a Source Connector
- D. Incorrect – JDBC Source ingests into Kafka
```

</details>

---

## Question 9

```markdown
Which of the following is a correct way to prefix Kafka topic names using Kafka Connect?
```

**Options**

```markdown
- A. Use the SMT RegexRouter with a replacement value
- B. Modify the Kafka topic directly in Zookeeper
- C. Patch the Kafka broker’s metadata service
- D. Use `kafka-topic-renamer.sh`
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
Kafka Connect allows prefixing topics using SMTs like RegexRouter.

- A. Correct – SMT + RegexRouter supports topic renaming
- B. Incorrect – Not how topic names are changed
- C. Incorrect – Brokers don’t have a renamer API
- D. Incorrect – No such script exists
```

</details>

---

## Question 10

```markdown
What is a major benefit of Kafka Connect over custom producer/consumer code?
```

**Options**

```markdown
- A. Better syntax highlighting
- B. Eliminates the need to write boilerplate integration code
- C. Enables schema inference from GraphQL
- D. Guarantees zero data loss
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Kafka Connect abstracts away boilerplate for data movement, requiring only configuration.

- A. Incorrect – Not relevant
- B. Correct – Kafka Connect minimizes code writing
- C. Incorrect – GraphQL is not related
- D. Incorrect – Fault tolerance helps, but zero data loss isn’t guaranteed
```

</details>

---

## Question 11

```markdown  
For a system designed to read data from an external database, perform some transformations, and then store the results in a Kafka topic, which approach is most suitable?  
```  

**Options**
```markdown  
- A. Consumer + Producer  
- B. Kafka Connect Source  
- C. Kafka Connect Sink  
- D. Kafka Streams  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
Kafka Connect Source is designed for importing data from external systems into Kafka topics.  

- A. Consumer + Producer requires custom coding and lacks built-in transformation capabilities.  
- B. Correct. Kafka Connect Source can read from databases and transform data before writing to Kafka.  
- C. Kafka Connect Sink exports data from Kafka to external systems.  
- D. Kafka Streams processes data already in Kafka, not external systems.  
```  

</details>  

---  

## Question 12

```markdown  
If the objective is to periodically export data from a Kafka topic to a relational database for long-term storage and analysis, which Kafka component would best fulfill this requirement?  
```  

**Options**
```markdown  
- A. Consumer + Producer  
- B. Kafka Connect Source  
- C. Kafka Connect Sink  
- D. Kafka Streams  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
Kafka Connect Sink connectors are specifically designed for exporting data from Kafka to external systems.  

- A. Consumer + Producer would require custom coding.  
- B. Kafka Connect Source imports data into Kafka.  
- C. Correct. Kafka Connect Sink handles database exports efficiently.  
- D. Kafka Streams processes data within Kafka.  
```  

</details>  

---  

## Question 13

```markdown  
A multinational corporation is looking to aggregate sales data from multiple regional systems into a centralized Kafka topic for real-time analysis and reporting. The regional systems vary in technology, including SQL databases and cloud-based storage solutions. Which solution enables the efficient and unified ingestion of these diverse data sources into Kafka?  
```  

**Options**
```markdown  
- A. Deploy Kafka Streams applications near each regional system to collect and forward data to the centralized Kafka topic.  
- B. Use Kafka Connect with a mix of Source Connectors suitable for each regional system's technology to ingest data directly into Kafka.  
- C. Implement custom Kafka Producers embedded within each regional system to push data to the centralized Kafka topic.  
- D. Configure a Kafka Connect Sink Connector for each regional system to replicate data into the centralized Kafka topic.  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
Kafka Connect's connector ecosystem handles diverse source systems efficiently.  

- A. Kafka Streams isn't designed for data ingestion.  
- B. Correct. Leverages Kafka Connect's connector ecosystem.  
- C. Custom solutions increase maintenance complexity.  
- D. Sink connectors move data out of Kafka, not into it.  
```  

</details>  

---  

## Question 14

```markdown  
An online media platform wishes to analyze user interactions (clicks, views, and comments) in real-time to dynamically adjust content recommendations. The platform generates a high volume of interaction data, necessitating scalable and real-time processing. What architecture best suits this requirement?  
```  

**Options**
```markdown  
- A. Utilize a Kafka Connect Source Connector to ingest interaction data into Kafka, then process this data with Kafka Streams to update content recommendations in real-time.  
- B. Directly stream interaction data into Kafka using a custom API, then use ksqlDB to perform real-time analysis and generate content recommendations.  
- C. Implement batch processing jobs to periodically analyze interaction data stored in an external database, and then use Kafka to distribute batch analysis results for content recommendation updates.  
- D. Configure Kafka Connect Sink Connectors to collect interaction data into a big data platform first, then process the data using external stream processing tools before updating content recommendations.  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
This architecture provides end-to-end real-time processing using Kafka-native components.  

- A. Correct. Full real-time pipeline using Kafka components.  
- B. Custom API bypasses Kafka Connect's benefits.  
- C. Batch processing doesn't meet real-time needs.  
- D. External processing adds unnecessary complexity.  
```  

</details>  

---  

## Question 15

```markdown  
A utility company monitors a network of IoT devices deployed across an energy grid. The devices send telemetry data (e.g., power usage, system health) every minute. The company wants to aggregate this data for near-real-time monitoring and anomaly detection. Which Kafka-based solution efficiently achieves this goal?  
```  

**Options**
```markdown  
- A. Configure Kafka Connect Sink Connectors to collect telemetry data from the IoT devices into Kafka, followed by a Kafka Streams application to aggregate and analyze the data.  
- B. Use Kafka Connect Source Connectors appropriate for the IoT devices' communication protocols to ingest telemetry data into Kafka, then employ Kafka Streams for data aggregation and anomaly detection.  
- C. Develop custom Kafka Producers within the IoT devices to send data directly to Kafka topics, then use external tools to pull and analyze the data from Kafka.  
- D. Implement a centralized database to collect IoT telemetry data first, then use Kafka Connect Source Connectors to ingest the data from the database into Kafka for further processing.  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
This solution provides the most direct and efficient path from devices to analysis.  

- A. Sink connectors move data out of Kafka, not into it.  
- B. Correct. Proper use of source connectors and stream processing.  
- C. Custom producers increase device complexity.  
- D. Database step adds unnecessary latency.  
```  

</details>  

--- 1

## Question 16

```markdown  
When needing to aggregate real-time data from a Kafka topic, compute running totals, and then publish those totals back to another Kafka topic for further analysis, which tool should you use?  
```  

**Options**
```markdown  
- A. Consumer + Producer  
- B. Kafka Connect Source  
- C. Kafka Connect Sink  
- D. Kafka Streams  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
Kafka Streams provides stateful operations for real-time aggregation and computation.  

- A. Consumer + Producer would require complex manual state management.  
- B. Kafka Connect Source is for importing external data.  
- C. Kafka Connect Sink exports data from Kafka.  
- D. Correct. Kafka Streams is designed for real-time stream processing.  
```  

</details>  

---  




----


Here's the **question-to-category mapping** for your quiz bank:

| Question #  | Category                          |
| ----------- | --------------------------------- |
| Question 1  | 1. what-is-kafka-connect.md       |
| Question 2  | *Not Kafka Connect related* (N/A) |
| Question 3  | 1. what-is-kafka-connect.md       |
| Question 4  | 10. Security in Kafka Connect.md  |
| Question 5  | 10. Security in Kafka Connect.md  |
| Question 6  | 10. Security in Kafka Connect.md  |
| Question 7  | 1. what-is-kafka-connect.md       |
| Question 8  | 1. what-is-kafka-connect.md       |
| Question 9  | 1. what-is-kafka-connect.md       |
| Question 10 | 6. Performance Tunning.md         |

Let me know if you'd like a version of this in CSV, or if you'd like help classifying additional questions.



## Question 2

```markdown
What is Kafka Connect, and what problem does it solve?
```

**Options**
```markdown
- A. It is a Kafka client for consuming messages.
- B. It is a tool for streaming data between Apache Kafka and other systems.
- C. It is a tool for mirroring data across Kafka clusters.
- D. It is a configuration management tool for Kafka.
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Kafka Connect simplifies the integration by providing a scalable, fault-tolerant, and configurable framework for reliably streaming data between Apache Kafka and various other data systems.
```

</details>

---

## Question 10

```markdown
What are the fundamental components of Kafka Connect?
```

**Options**
```markdown
- A. Workers
- B. Tasks
- C. Consumers
- D. Connectors
```

<details><summary>Response:</summary>

**Answer:** A, B, D

**Explanation:**

```markdown
Kafka Connect is built on three main components: Connectors, which are reusable Java JARs configured by users; Tasks, which handle data import/export; and Workers, Java processes that execute tasks either standalone or clustered.
```

</details>

--- 