## Question 1

```markdown  
Which protocol is recommended for encrypting communication between Kafka Connect workers and brokers?  
```  

**Options**
```markdown  
- A. PLAINTEXT  
- B. SASL_PLAINTEXT  
- C. SSL  
- D. HTTP  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
SSL (or TLS) is the standard for encrypting data in transit between workers and brokers.  

- A. PLAINTEXT is insecure and should never be used in production.  
- B. SASL_PLAINTEXT provides authentication but no encryption.  
- C. Correct. SSL ensures encrypted communication.  
- D. HTTP is unrelated to broker-worker security.  
```  

</details>  

---  

## Question 2

```markdown  
What is the purpose of the `ssl.endpoint.identification.algorithm=HTTPS` setting?  
```  

**Options**
```markdown  
- A. Disables hostname verification  
- B. Enforces strict certificate hostname matching  
- C. Allows self-signed certificates  
- D. Switches to HTTP basic auth  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
This setting ensures the broker's certificate matches its hostname, preventing MITM attacks.  

- A. Disabling verification requires `=""`, which is insecure.  
- B. Correct. HTTPS validation enforces hostname checks.  
- C. Self-signed certs work but still require hostname verification.  
- D. HTTP auth is unrelated to SSL configuration.  
```  

</details>  

---  

## Question 3

```markdown  
Which SASL mechanism is recommended for password-based authentication with salting?  
```  

**Options**
```markdown  
- A. PLAIN  
- B. SCRAM-SHA-256  
- C. GSSAPI  
- D. OAUTHBEARER  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
SCRAM provides salted password hashing, unlike PLAIN which sends credentials in clear text.  

- A. PLAIN is insecure for production.  
- B. Correct. SCRAM is the secure choice for password auth.  
- C. GSSAPI is for Kerberos, not password auth.  
- D. OAUTHBEARER uses tokens, not passwords.  
```  

</details>  

---  

## Question 4

```markdown  
How should secrets like keystore passwords be managed in production?  
```  

**Options**
```markdown  
- A. Hardcoded in properties files  
- B. Stored in version control  
- C. Using environment variables or Vault  
- D. Plaintext in connector configs  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
Environment variables or secret managers (e.g., Vault) prevent exposure of sensitive data.  

- A. Hardcoding is a security risk.  
- B. Version control is inappropriate for secrets.  
- C. Correct. Secure secret management is critical.  
- D. Plaintext is never acceptable.  
```  

</details>  

---  

## Question 5

```markdown  
Which role allows creating and deleting connectors in Confluent RBAC?  
```  

**Options**
```markdown  
- A. ConnectDeveloper  
- B. ConnectOperator  
- C. ConnectAdmin  
- D. ConnectUser  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
The `ConnectAdmin` role has full management privileges.  

- A. Developers can only view/restart connectors.  
- B. Operators can pause/resume but not create/delete.  
- C. Correct. Admin role has full control.  
- D. Not a standard RBAC role.  
```  

</details>  

---  

## Question 6

```markdown  
What is the correct JAAS configuration for SCRAM authentication?  
```  

**Options**
```markdown  
- A. `org.apache.kafka.common.security.plain.PlainLoginModule`  
- B. `org.apache.kafka.common.security.scram.ScramLoginModule`  
- C. `org.apache.kafka.common.security.kerberos.KerberosLoginModule`  
- D. `org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule`  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
SCRAM requires the `ScramLoginModule` for proper salted password handling.  

- A. Used for PLAIN mechanism, not SCRAM.  
- B. Correct. Matches the file's example.  
- C. Used for Kerberos (GSSAPI).  
- D. Used for OAuth tokens.  
```  

</details>  

---  

## Question 7

```markdown  
Which port should be open for inbound Connect REST API access?  
```  

**Options**
```markdown  
- A. 9092  
- B. 8083  
- C. 2181  
- D. 9093  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
The REST API defaults to port 8083 for management operations.  

- A. 9092 is the default insecure Kafka port.  
- B. Correct. REST API uses 8083.  
- C. 2181 is for Zookeeper.  
- D. 9093 is for SSL-enabled Kafka.  
```  

</details>  

---  

## Question 8

```markdown  
What does the `verify-full` SSL mode enforce for JDBC connections?  
```  

**Options**
```markdown  
- A. No certificate verification  
- B. Certificate validation only  
- C. Certificate + hostname verification  
- D. Password encryption only  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
`verify-full` ensures both certificate validity and hostname matching.  

- A. Describes `disable` or `allow` modes.  
- B. `verify-ca` skips hostname checks.  
- C. Correct. Full verification is most secure.  
- D. SSL modes don't handle password encryption.  
```  

</details>  

---  

## Question 9

```markdown  
Which configuration secures the Connect REST API with HTTP Basic Auth?  
```  

**Options**
```markdown  
- A. `rest.extension.classes=BasicAuthSecurityRestExtension`  
- B. `security.protocol=SASL_SSL`  
- C. `ssl.keystore.location=/path/to/keystore.jks`  
- D. `sasl.mechanism=SCRAM-SHA-256`  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
The `BasicAuthSecurityRestExtension` enables HTTP Basic Auth for the REST API.  

- A. Correct. Matches the file's example.  
- B. Secures broker connections, not REST API.  
- C. Configures SSL certificates, not REST auth.  
- D. Configures SASL for brokers.  
```  

</details>  

---  

## Question 10

```markdown  
What is the first step to troubleshoot `SASL authentication failed` errors?  
```  

**Options**
```markdown  
- A. Disable SASL and use SSL only  
- B. Check JAAS config and Kafka ACLs  
- C. Restart all Connect workers  
- D. Increase `retry.backoff.ms`  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
Misconfigured JAAS or missing ACLs are common causes of SASL failures.  

- A. Disabling security is not a solution.  
- B. Correct. Verify credentials and permissions first.  
- C. Restarts won't fix configuration errors.  
- D. Retry settings don't resolve auth issues.  
```  

</details>  

---  

## Question 11

```markdown  
A financial institution wants to analyze transaction data in real-time to detect fraudulent activities. The transaction data, which includes sensitive information, is initially stored in a mainframe system. Which approach ensures secure, real-time analysis of this data by a Kafka Streams application while complying with data privacy regulations?  
```  

**Options**
```markdown  
- A. Utilize a mainframe connector with Kafka Connect to ingest the transaction data into a Kafka topic. Apply a Kafka Streams application to anonymize sensitive information in the stream before conducting fraud analysis.  
- B. Directly connect the mainframe system to the Kafka Streams application using a custom API, ensuring sensitive data is filtered out during the streaming process.  
- C. Employ ksqlDB to directly query the mainframe system, apply data anonymization functions to filter out sensitive information, and then write the sanitized data to a Kafka topic for stream processing.  
- D. Implement a batch process to extract transaction data periodically from the mainframe, cleanse the data of sensitive information, and then load the sanitized data into Kafka topics for streaming analysis.  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
This approach leverages Kafka Connect for secure ingestion and Kafka Streams for real-time processing with anonymization.  

- A. Correct. Uses proper Kafka components while addressing privacy requirements.  
- B. Custom API bypasses Kafka's built-in security features.  
- C. ksqlDB isn't designed for direct mainframe connectivity.  
- D. Batch processing doesn't meet real-time requirements.  
```  

</details>  

---  

## Question 12

```markdown  
Your organization aims to implement a real-time recommendation engine for e-commerce users based on their browsing behavior. User session data is considered sensitive and must be anonymized before processing. How can the Kafka ecosystem be leveraged to meet these requirements?  
```  

**Options**
```markdown  
- A. Deploy a Kafka Connect Source Connector to capture session data directly into Kafka, using Stream Processing to anonymize user identifiers before aggregating sessions for recommendation analysis.  
- B. Use a custom Kafka Producer application to publish session data to a topic, applying a Stream Processor to anonymize and then process the data for generating recommendations.  
- C. Implement a Kafka Connect Sink Connector to store session data into a NoSQL database, with a pre-processor to remove sensitive information before ingestion. Use Kafka Streams to read from the database for analysis.  
- D. Create a ksqlDB process to pull session data from source systems, apply anonymization functions within ksqlDB, and output the clean data to a Kafka topic for further processing by the Kafka Streams application.  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
This solution uses Kafka's native components for end-to-end secure processing.  

- A. Correct. Leverages Kafka Connect for ingestion and Kafka Streams for processing/anonymization.  
- B. Custom producers increase maintenance overhead.  
- C. Unnecessary database step adds latency.  
- D. ksqlDB isn't ideal for direct data ingestion.  
```  

</details>  

---  

(Continuing with the same format for questions 6-10...)

---

## Question 13

```markdown  
An organization is implementing a system to monitor and alert on infrastructure health status in real-time. The system collects metrics from various sources, including some that contain proprietary information. Which approach ensures that only non-proprietary, critical health metrics are analyzed and alerted on?  
```  

**Options**
```markdown  
- A. Use Kafka Connect with appropriate Source Connectors for each metric source, configuring the connectors to filter out proprietary information. Process the filtered metrics stream with Kafka Streams for alerting.  
- B. Directly stream all metrics into Kafka using custom Producers, then employ a Kafka Streams application to separate proprietary data from non-proprietary data, and analyze the latter for alerting.  
- C. Implement a series of ksqlDB statements to ingest metrics into Kafka, applying filtering logic within ksqlDB to remove proprietary information before streaming processing and alerting.  
- D. Configure a Kafka Connect Sink Connector to aggregate all metrics into a centralized database, followed by batch processing to remove proprietary information before streaming the data into Kafka for real-time analysis.  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
Filtering at the source connector level is most efficient for this use case.  

- A. Correct. Early filtering reduces processing overhead.  
- B. Processes unnecessary proprietary data first.  
- C. ksqlDB filtering occurs after all data enters Kafka.  
- D. Batch processing defeats real-time requirements.  
```  

</details>  

---  