## Question 1

```markdown
Which field in a Kafka Connector JSON config specifies the class to be used for the connector?
```

**Options**

```markdown
- A. connector.name
- B. connector.type
- C. connector.class
- D. class.name
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Not a valid field
- B. Incorrect naming
- C. ✅ Correct – This defines which Java class Kafka Connect should use
- D. Not a recognized Kafka Connect field
```

</details>

---

## Question 2

```markdown
What is the purpose of the `auto.create` field in a JDBC Sink connector?
```

**Options**

```markdown
- A. Automatically starts Kafka Connect when the cluster boots
- B. Creates Kafka topics if they don’t exist
- C. Automatically creates database tables if they don’t exist
- D. Automatically evolves Kafka schema
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Connect starts independently
- B. Topic creation is handled by Kafka, not the connector
- C. ✅ Correct – `auto.create: true` allows table creation
- D. Schema evolution is handled separately via `auto.evolve`
```

</details>

---

## Question 3

```markdown
What is the function of the `flush.size` parameter in the JDBC Sink connector configuration?
```

**Options**

```markdown
- A. Number of records to buffer in Kafka
- B. Size in bytes to flush logs
- C. Number of records after which to flush data to the database
- D. Time interval to commit offsets
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Buffering in Kafka is not related
- B. Not related to log flushing
- C. ✅ Correct – `flush.size` controls when to write batched data to the sink
- D. Offset commits are separate from this behavior
```

</details>

---

## Question 4

```markdown
Which of the following connectors is designed to stream data from Kafka to Amazon S3?
```

**Options**

```markdown
- A. JDBC Sink Connector
- B. S3 Sink Connector
- C. SFTP Source Connector
- D. JMS Source Connector
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Sends to relational databases
- B. ✅ Correct – S3 Sink pushes data to Amazon S3
- C. Reads data from SFTP into Kafka
- D. Reads messages from JMS into Kafka
```

</details>

---

## Question 5

```markdown
Which of the following tools can be used to submit a connector configuration to Kafka Connect?
```

**Options**

```markdown
- A. Kafka Streams API
- B. Kafka Connect UI
- C. Kafka REST Proxy
- D. curl with Kafka Connect REST API
```

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**

```markdown
- A. Kafka Streams is for processing
- B. There is no official Kafka Connect UI
- C. REST Proxy is for producing/consuming, not connector management
- D. ✅ Correct – You can POST the config using curl to the Kafka Connect REST API
```

</details>

---

## Question 6

```markdown
What is the correct `insert.mode` value to configure insert-only behavior in JDBC Sink Connector?
```

**Options**

```markdown
- A. upsert
- B. append
- C. insert
- D. write
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Upsert combines insert + update logic
- B. Not a valid insert.mode
- C. ✅ Correct – `insert` ensures new rows are added
- D. Not a supported option
```

</details>

---

## Question 7

```markdown
Which configuration specifies the Kafka topics used by a connector?
```

**Options**

```markdown
- A. kafka.source
- B. input.topics
- C. topics
- D. kafka.topics
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Invalid field
- B. Not a recognized configuration
- C. ✅ Correct – `topics` defines the Kafka topics used
- D. Not an official config key
```

</details>

---

## Question 8

```markdown
Which of the following is NOT a valid troubleshooting step if a connector fails?
```

**Options**

```markdown
- A. Check connector logs
- B. Verify the Kafka topic exists
- C. Restart Kafka Connect service with `kafka-topics.sh`
- D. Use REST API to check connector status
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. ✅ Recommended for error insights
- B. ✅ Important to verify source/sink topics
- C. ❌ Incorrect – `kafka-topics.sh` is unrelated to Kafka Connect service
- D. ✅ REST API helps monitor connector state
```

</details>

---

## Question 9

```markdown
What does the `auto.evolve` setting enable in JDBC Sink connectors?
```

**Options**

```markdown
- A. Automatic creation of Kafka topics
- B. Schema evolution in destination database tables
- C. Updating worker configurations on restart
- D. Auto-scaling of Kafka Connect workers
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Kafka topic creation is separate
- B. ✅ Correct – `auto.evolve: true` lets schemas evolve with new fields
- C. Not related to worker behavior
- D. Scaling is manual or orchestrated separately
```

</details>

---

## Question 10

```markdown
What is the key difference between connector config and worker config in Kafka Connect?
```

**Options**

```markdown
- A. Connector config defines logging levels
- B. Worker config specifies external DBs to connect to
- C. Connector config controls connector behavior, worker config controls the execution environment
- D. Worker config defines Kafka topic schemas
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Logging is part of worker setup, but not connector config
- B. External system connection is defined in connector config
- C. ✅ Correct – Connector config = task behavior, Worker config = Kafka Connect environment
- D. Topic schemas are not defined in worker config
```

</details>

---

## Question 11

```markdown  
When using plain JSON data with Connect, you see the following error message:  
org.apache.kafka.connect.errors.DataException: JsonDeserializer with schemas.enable requires "schema" and "payload"  
fields and may not contain additional fields. How will you fix the error?  
```  

**Options**
```markdown  
- A. Set key.converter.schema.enable = true and value.converter.schemas.enable to false  
- B. Set value.converter.schemas.enable to Avro converter  
- C. Set value.converter.schemas.enable to Json converter  
- D. Set key.converter.schemas.enable to false  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
The error occurs when schema validation is enabled for plain JSON data.  

- A. Correct. Disabling schema validation for values fixes this error  
- B. Switching to Avro doesn't solve the JSON schema requirement  
- C. This maintains the same schema validation issue  
- D. Only addresses key conversion, not the value error  
```  

</details>  

---  

## Question 12

```markdown  
You are using JDBC source connector to copy data from 3 tables to three Kafka topics.  
There is one connector created with max.tasks equal to 2 deployed on a cluster of 3 workers.  
How many tasks are launched?  
```  

**Options**
```markdown  
- A. 1  
- B. 3  
- C. 6  
- D. 2  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
The number of tasks is determined by max.tasks configuration.  

- A. Would underutilize the configured capacity  
- B. Exceeds the max.tasks limit  
- C. Incorrectly multiplies tables and workers  
- D. Correct. The connector creates tasks up to max.tasks limit  
```  

</details>  

---

## Question 13

```markdown  
Where are the Kafka Connect connector configurations stored?  
```  

**Options**
```markdown  
- A. In a separate config file on each Kafka Connect worker  
- B. In the Kafka broker's config directory  
- C. In Zookeeper under the `/kafka-connect` znode  
- D. In a special Kafka topic named `connect-configs`  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
Kafka Connect uses internal topics for configuration storage.  

- A. Configs are not worker-local  
- B. Brokers don't store connector configs  
- C. Zookeeper is not used for this purpose  
- D. Correct. Configs are stored in Kafka topics  
```  

</details>  

---  

## Question 14

```markdown  
You are using a JDBC source connector to copy data from a database table to a Kafka topic. The table has 5 columns. How many tasks will be created by the connector?  
```  

**Options**
```markdown  
- A. 1  
- B. 5  
- C. It depends on the `max.tasks` configuration of the connector  
- D. It depends on the number of partitions in the Kafka topic  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
JDBC connectors create one task per table by default.  

- A. Correct. Number of columns doesn't affect task count  
- B. Tasks aren't created per column  
- C. `max.tasks` doesn't override per-table behavior  
- D. Partition count doesn't affect source task count  
```  

</details>  

---  

## Question 15

```markdown  
What happens if the `max.tasks` configuration is set to a value less than the number of tables being copied by a JDBC source connector?  
```  

**Options**
```markdown  
- A. The connector will create one task per table, ignoring the `max.tasks` setting  
- B. The connector will create tasks up to the `max.tasks` limit, potentially leaving some tables without dedicated tasks  
- C. The connector will distribute the tables evenly among the available tasks  
- D. The connector will fail with an error due to the insufficient number of tasks  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
The connector intelligently shares tables across tasks.  

- A. `max.tasks` is always respected  
- B. No tables are left unprocessed  
- C. Correct. Tables are load-balanced across tasks  
- D. The connector handles this scenario gracefully  
```  

</details>  

---  






## Question 6

```markdown
Why are Config Definitions important in Kafka Connect?
```

**Options**
```markdown
- A. They dictate the security protocols used by connectors.
- B. They define the required and optional settings for each connector.
- C. They provide default values for connector configurations.
- D. They help manage connector scalability and performance.
```

<details><summary>Response:</summary>

**Answer:** B, C

**Explanation:**

```markdown
Config Definitions in Kafka Connect are critical because they specify what settings are available for each connector, including required and optional parameters, and provide default values that help streamline connector setup. This ensures that connectors are configured properly to operate effectively and securely within a Kafka ecosystem.
```

</details>

---



## Question 8

```markdown
What are key considerations when configuring Kafka Connect?
```

**Options**
```markdown
- A. Setting up connection details for the source or sink system.
- B. Assigning consumer groups for data processing.
- C. Configuring topics for data flow.
- D. Choosing the right connector plugins.
```

<details><summary>Response:</summary>

**Answer:** A, C, D

**Explanation:**

```markdown
Proper configuration of Kafka Connect involves selecting appropriate connector plugins, setting up connection details, and configuring topics to ensure efficient and correct data flow.
```

</details>

---

