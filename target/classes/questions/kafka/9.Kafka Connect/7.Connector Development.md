## Question 1

```markdown
What is the primary purpose of the `Connector` interface in Kafka Connect?  
```

**Options**
```markdown
- A. To perform the actual data movement  
- B. To define the behavior of the connector and manage tasks  
- C. To validate configuration settings  
- D. To package the connector into a JAR  
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
The `Connector` interface defines the behavior of the connector and manages tasks, including specifying the task class and generating task configurations.  

- A. This is the role of the `Task` interface.  
- B. Correct. The `Connector` interface handles high-level connector behavior.  
- C. This is the role of `ConfigDef`.  
- D. Packaging is a build/deployment step, not handled by the `Connector` interface.  
```

</details>  

---  

## Question 2

```markdown
Which method in the `Task` interface is responsible for fetching data in a source connector?  
```  

**Options**
```markdown  
- A. `start()`  
- B. `put()`  
- C. `poll()`  
- D. `stop()`  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
In source connectors, the `poll()` method fetches data and returns `SourceRecord` objects.  

- A. `start()` initializes the task but does not fetch data.  
- B. `put()` is used in sink connectors to write data.  
- C. Correct. `poll()` is the primary method for data retrieval in source tasks.  
- D. `stop()` cleans up resources but does not fetch data.  
```  

</details>  

---  

## Question 3

```markdown  
What is the purpose of the `ConfigDef` class in Kafka Connect?  
```  

**Options**
```markdown  
- A. To define the JAR packaging rules  
- B. To validate and enforce configuration settings  
- C. To manage task execution  
- D. To handle network communication  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
`ConfigDef` validates and enforces configuration rules, ensuring required fields are present and values are correct.  

- A. Packaging is handled by build tools like Maven.  
- B. Correct. `ConfigDef` validates configurations.  
- C. Task execution is managed by the `Task` interface.  
- D. Network communication is handled by Kafka Connect internals.  
```  

</details>  

---  

## Question 4

```markdown  
Which Maven plugin is used to build a JAR with dependencies for a Kafka connector?  
```  

**Options**
```markdown  
- A. `maven-compiler-plugin`  
- B. `maven-shade-plugin`  
- C. `maven-surefire-plugin`  
- D. `maven-deploy-plugin`  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
The `maven-shade-plugin` packages the connector and its dependencies into a single JAR.  

- A. This plugin compiles code but does not bundle dependencies.  
- B. Correct. The `shade` plugin creates an uber-JAR.  
- C. This plugin runs tests, not packaging.  
- D. This plugin deploys artifacts but does not bundle dependencies.  
```  

</details>  

---  

## Question 5

```markdown  
Where should you place the connector JAR for Kafka Connect to recognize it?  
```  

**Options**
```markdown  
- A. In the `bin` directory of Kafka  
- B. In the `plugin.path` directory  
- C. In the `config` directory  
- D. In the `logs` directory  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
The `plugin.path` directory is where Kafka Connect looks for connector JARs.  

- A. The `bin` directory contains scripts, not plugins.  
- B. Correct. `plugin.path` is the designated location.  
- C. The `config` directory stores configuration files.  
- D. The `logs` directory contains log files.  
```  

</details>  

---  

## Question 6

```markdown  
What is the purpose of the `MockConnect` framework?  
```  

**Options**
```markdown  
- A. To simulate a Kafka cluster for performance testing  
- B. To validate connectors without a full Kafka cluster  
- C. To package connectors into Docker images  
- D. To monitor connector metrics  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
`MockConnect` allows testing connectors in isolation without requiring a live Kafka cluster.  

- A. It is for functional testing, not performance.  
- B. Correct. It validates connectors in a mock environment.  
- C. Docker packaging is unrelated to `MockConnect`.  
- D. Metrics are handled by Kafka Connect's monitoring tools.  
```  

</details>  

---  

## Question 7

```markdown  
Which method in the `Connector` interface determines how many tasks will be created?  
```  

**Options**
```markdown  
- A. `taskClass()`  
- B. `taskConfigs()`  
- C. `config()`  
- D. `start()`  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
`taskConfigs(int maxTasks)` generates configurations for each task, defining the parallelism.  

- A. `taskClass()` specifies the task implementation class.  
- B. Correct. `taskConfigs()` controls task count and configurations.  
- C. `config()` returns general connector settings.  
- D. `start()` initializes the connector but does not manage tasks.  
```  

</details>  

---  

## Question 8

```markdown  
What is a best practice for sink connectors to avoid duplicate data writes?  
```  

**Options**
```markdown  
- A. Use incremental polling  
- B. Implement idempotent writes  
- C. Enable debug logging  
- D. Restart workers frequently  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
Idempotent writes ensure that duplicate operations do not result in duplicate data.  

- A. Incremental polling is for source connectors.  
- B. Correct. Idempotency prevents duplicates in sinks.  
- C. Debug logging helps troubleshoot but does not solve duplicates.  
- D. Restarting workers is not a solution for idempotency.  
```  

</details>  

---  

## Question 9

```markdown  
Which file is used to configure a standalone Kafka Connect worker?  
```  

**Options**
```markdown  
- A. `connector.properties`  
- B. `worker.properties`  
- C. `config.properties`  
- D. `connect.properties`  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
`worker.properties` defines settings like `plugin.path` and broker connections for standalone mode.  

- A. `connector.properties` configures the connector, not the worker.  
- B. Correct. `worker.properties` is for standalone worker configuration.  
- C. `config.properties` is not a standard Kafka Connect file.  
- D. `connect.properties` is not a standard file.  
```  

</details>  

---  

## Question 10

```markdown  
What is the first step in debugging a misbehaving connector?  
```  

**Options**
```markdown  
- A. Check worker logs for `ERROR`/`WARN` messages  
- B. Restart the Kafka cluster  
- C. Rebuild the connector JAR  
- D. Increase the number of tasks  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
Worker logs often contain actionable error messages or warnings.  

- A. Correct. Logs are the primary source of debugging information.  
- B. Restarting the cluster is unnecessary for most connector issues.  
- C. Rebuilding the JAR is unlikely to solve runtime issues.  
- D. Increasing tasks may exacerbate the problem.  
```  

</details>  

---

## Question 11

```markdown
Evaluate this schema definition for Kafka Connect. What is its purpose in a data integration pipeline?

public static Schema USER_SCHEMA = SchemaBuilder.struct()
.name("user-schema")
.version(1)
.field("user_name", Schema.STRING_SCHEMA)
.field("user_id", Schema.INT32_SCHEMA)
.field("user_city", Schema.STRING_SCHEMA)
.build();
```

**Options**
```markdown
- A. Encrypts user data before writing to Kafka for enhanced security.
- B. Transforms data from JSON to Avro format for efficient serialization.
- C. Defines the data structure for a user entity to ensure consistent data formatting.
- D. Filters out sensitive user information from being transmitted.
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
This schema definition specifies the structure and types of data fields for a user entity, crucial for maintaining data integrity and consistency across systems during integration tasks.
```

</details>

---

## Question 12

```markdown
Consider the following Java code snippet for defining a Kafka Connector Schema. Identify the issue in the code and explain how to fix it.

public static Schema USER_SCHEMA = SchemaBuilder.struct()
.name(SCHEMA_VALUE_USER)
.version(1)
.field(USER_WEBSITE_FIELD, Schema.STRING)
.field(USER_ID_FIELD, Schema.INT32)
.field(USER_NAME_FIELD, Schema.STRING)
.build();
```

**Options**
```markdown
- A. The Schema.STRING and Schema.INT32 should be replaced with Schema.STRING_SCHEMA and Schema.INT32_SCHEMA, respectively.
- B. The USER_ID_FIELD should use Schema.INT64 instead of Schema.INT32.
- C. The version method should not be used in the schema definition.
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
The issue in the provided Java code snippet is that the schema types Schema.STRING and Schema.INT32 are incorrectly specified. They should be replaced with Schema.STRING_SCHEMA and Schema.INT32_SCHEMA, respectively. The corrected schema definition should be:

public static Schema USER_SCHEMA = SchemaBuilder.struct()
.name(SCHEMA_VALUE_USER)
.version(1)
.field(USER_WEBSITE_FIELD, Schema.STRING_SCHEMA)
.field(USER_ID_FIELD, Schema.INT32_SCHEMA)
.field(USER_NAME_FIELD, Schema.STRING_SCHEMA)
.build();
```

</details>

---