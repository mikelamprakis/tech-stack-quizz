## Question 1

```markdown
What does Serde stand for in Kafka Streams?
```

**Options**

```markdown
- A. Serializer/Deserializer
- B. Service/Delivery
- C. Stream/Database
- D. Session/Delegate
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
- A. Correct. Serde stands for Serializer/Deserializer.
- B. Incorrect.
- C. Incorrect.
- D. Incorrect.
```

</details>

---

## Question 2

```markdown
Why are Serdes important in Kafka Streams applications?
```

**Options**

```markdown
- A. They control how data is converted to/from byte arrays for Kafka topics
- B. They manage message retries automatically
- C. They handle windowing logic
- D. They perform repartitioning of streams
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
Serdes handle serialization (to bytes) and deserialization (from bytes) for records when reading/writing to Kafka.

- A. Correct.
- B. Retry logic is separate.
- C. Windowing is unrelated.
- D. Repartitioning is a different concern.
```

</details>

---

## Question 3

```markdown
Which of the following is NOT a built-in Kafka Streams Serde?
```

**Options**

```markdown
- A. Serdes.String()
- B. Serdes.Integer()
- C. Serdes.JSON()
- D. Serdes.Long()
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
Kafka Streams provides built-in Serdes for primitive types like String, Integer, Long but not JSON.

- A. Built-in.
- B. Built-in.
- C. No built-in JSON serde; must implement or use third-party.
- D. Built-in.
```

</details>

---

## Question 4

```markdown
How can you use a custom Serde in a Kafka Streams application?
```

**Options**

```markdown
- A. Implement the Serializer and Deserializer interfaces and wrap them in a Serde
- B. Use the built-in JSON Serde directly
- C. Only use Strings as keys and values
- D. You cannot use custom Serdes
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
Custom Serdes require implementing Serializer and Deserializer interfaces and combining them in a Serde implementation.

- A. Correct.
- B. No built-in JSON serde.
- C. Limiting.
- D. Incorrect.
```

</details>

---

## Question 5

```markdown
What Serde should you typically use for the key in a Kafka Streams application?
```

**Options**

```markdown
- A. Serdes.ByteArray()
- B. Serdes.String()
- C. Serdes.Long()
- D. Any Serde that matches the key data type
```

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**

```markdown
Keys can have different types; you should use the Serde matching the key type.

- A. Possible but not typical.
- B. Common for string keys.
- C. For numeric keys.
- D. Correct.
```

</details>

---

## Question 6

```markdown
Which Kafka Streams API method is used to specify Serdes for keys and values when building a stream?
```

**Options**

```markdown
- A. `Consumed.with(keySerde, valueSerde)`
- B. `Produced.with(keySerde, valueSerde)`
- C. `StreamsBuilder.withSerdes()`
- D. `StreamConfig.setSerdes()`
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
`Consumed.with()` specifies Serdes when consuming from a topic.

- A. Correct.
- B. Used for producing.
- C. Does not exist.
- D. No such method.
```

</details>

---

## Question 7

```markdown
What happens if Kafka Streams tries to deserialize a record with the wrong Serde?
```

**Options**

```markdown
- A. The record is skipped silently
- B. A SerializationException is thrown
- C. The record is converted to a default value
- D. The application crashes immediately
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Deserialization with the wrong Serde will cause a SerializationException to be thrown.

- A. Incorrect.
- B. Correct.
- C. Incorrect.
- D. May crash if exception not handled but not necessarily immediate.
```

</details>

---

## Question 8

```markdown
Is it possible to change the Serde configuration dynamically at runtime in Kafka Streams?
```

**Options**

```markdown
- A. Yes, Serdes can be changed at any time during execution
- B. No, Serdes are configured at topology build time and cannot change dynamically
- C. Only if using specific Kafka Streams versions
- D. Only for keys, not values
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
Serdes are set when building the topology and cannot be changed dynamically at runtime.

- A. Incorrect.
- B. Correct.
- C. No version dependency.
- D. Incorrect.
```

</details>

---

## Question 9

```markdown
What is the default Serde for keys and values in Kafka Streams if none is specified?
```

**Options**

```markdown
- A. Serdes.String()
- B. Serdes.ByteArray()
- C. No default; serialization errors occur
- D. Serdes.Bytes()
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
By default, Kafka Streams uses Serdes.String() for both keys and values if none is specified.

- A. Correct.
- B. Incorrect.
- C. Incorrect.
- D. Incorrect.
```

</details>

---

## Question 10

```markdown
How can you configure Serde behavior like schema evolution or compatibility when using Avro or Protobuf?
```

**Options**

```markdown
- A. By using specific Serdes that support schema registries
- B. By setting Kafka Streams properties only
- C. By implementing custom serializers only
- D. By using built-in JSON Serde
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
Using Serdes integrated with schema registries (like Confluent Schema Registry) enables schema evolution and compatibility.

- A. Correct.
- B. Properties alone are insufficient.
- C. Custom serializers may be needed but integration is key.
- D. JSON Serde does not handle schema compatibility.
```

</details>



## Question 11

```markdown
You are implementing a Kafka Streams application. The input is a KStream from a topic where the message values are in Avro format. What should you set for the `default.value.serde` property in the Streams configuration?
```

**Options**

```markdown
- A. `Serdes.String()`
- B. `Serdes.ByteArray()`
- C. `SpecificAvroSerde`
- D. `GenericAvroSerde`
```


<details><summary>Response:</summary> 

**Answer:** C

**Explanation:**

```markdown
In a Kafka Streams application, you need to specify the default serdes (serializers/deserializers) for message keys and values. Since the input topic has message values in Avro format, you should set:

- `default.value.serde=SpecificAvroSerde`: This tells Kafka Streams to use the `SpecificAvroSerde` to deserialize the message values. This assumes that you have specific Avro-generated classes for your data schema.

The other options are not ideal:

- A: `Serdes.String()` would be incorrect because the message values are in Avro format, not plain strings.
- B: `Serdes.ByteArray()` could work, but it would give you raw bytes that you'd have to deserialize manually. It's better to use a specific Avro serde.
- D: `GenericAvroSerde` could be used if you don't have specific Avro-generated classes and want to use the generic Avro record representation. But if you have specific classes, `SpecificAvroSerde` is preferred.
```

</details>

---

## Question 12

```markdown
When developing a Kafka Streams application that aggregates transaction amounts by user ID from a financial transactions topic, which key-serde configuration ensures optimal processing and state store management?

StreamsBuilder builder = new StreamsBuilder();
KStream<String, BigDecimal> transactions = builder.stream("transactions-topic",
    Consumed.with(Serdes.String(), Serdes.BigDecimal()));
KTable<String, BigDecimal> aggregatedTransactions = transactions
    .groupByKey()
    .reduce(BigDecimal::add, Materialized.as("aggregated-transactions-store"));
aggregatedTransactions.toStream().to("aggregated-transactions-topic");
builder.build();
```

**Options**

```markdown
- A. Serdes.String(), Serdes.BigDecimal()
- B. Serdes.Long(), Serdes.Double()
- C. Serdes.String(), Serdes.String()
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
The key is a user ID (String), and the value is monetary (BigDecimal). Using the proper Serdes ensures correct serialization and deserialization.

- A. Correct. Matches data structure.
- B. Wrong types for both key and value.
- C. Treats BigDecimal as String â€” unsafe.
```

</details>

---