## Question 1

```markdown
What is the primary role of a Kafka Connect source connector?
```

**Options**

```markdown
- A. To push data from Kafka to an external system
- B. To ingest data from Kafka into a database
- C. To ingest data from external systems into Kafka
- D. To index Kafka messages into Elasticsearch
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. This is the role of a sink connector.
- B. This reverses the data flow of a source.
- C. ✅ Correct – Source connectors pull data *into* Kafka.
- D. This describes a sink connector like the Elasticsearch Sink.
```

</details>

---

## Question 2

```markdown
Which of the following is an example of a Kafka Connect sink connector?
```

**Options**

```markdown
- A. File Source Connector
- B. JDBC Source Connector
- C. MQTT Source Connector
- D. Elasticsearch Sink Connector
```

<details><summary>Response:</summary>

**Answer:** D

**Explanation:**

```markdown
- A. File Source Connector pulls data *into* Kafka.
- B. JDBC Source is also for ingesting data *into* Kafka.
- C. MQTT is a source for IoT data.
- D. ✅ Correct – Elasticsearch Sink pushes data *out of* Kafka into Elasticsearch.
```

</details>

---

## Question 3

```markdown
What best describes the relationship between Kafka and a sink connector?
```

**Options**

```markdown
- A. Kafka consumes data from the sink
- B. Kafka transforms data for the sink
- C. Kafka provides data to the sink
- D. Kafka monitors the sink system
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Sink connectors consume from Kafka, not the other way around.
- B. Transformation can happen, but it's not the defining trait.
- C. ✅ Correct – Kafka is the source for sink connectors.
- D. Kafka Connect doesn't monitor systems automatically.
```

</details>

---

## Question 4

```markdown
Which connector would you use to stream data from IoT devices into Kafka?
```

**Options**

```markdown
- A. S3 Sink Connector
- B. JDBC Sink Connector
- C. MQTT Source Connector
- D. Elasticsearch Sink Connector
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Writes to Amazon S3, not relevant to IoT devices.
- B. Pushes data to databases.
- C. ✅ Correct – MQTT Source ingests real-time IoT data.
- D. Used for analytics, not ingestion.
```

</details>

---

## Question 5

```markdown
In a data pipeline from PostgreSQL to Elasticsearch, which connector reads the data from PostgreSQL?
```

**Options**

```markdown
- A. Elasticsearch Sink Connector
- B. Kafka Consumer
- C. JDBC Source Connector
- D. JDBC Sink Connector
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. This writes to Elasticsearch.
- B. Not a connector – Kafka consumers are separate.
- C. ✅ Correct – JDBC Source pulls data from PostgreSQL.
- D. Sink connectors write *to* a DB, not read from it.
```

</details>

---

## Question 6

```markdown
Which connector would be used to store Kafka data into Amazon S3?
```

**Options**

```markdown
- A. S3 Source Connector
- B. S3 Sink Connector
- C. File Sink Connector
- D. JDBC Sink Connector
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. There is no standard S3 Source Connector.
- B. ✅ Correct – S3 Sink pushes Kafka data into S3.
- C. File connectors typically read from disk, not S3.
- D. JDBC works with relational databases, not S3.
```

</details>

---

## Question 7

```markdown
In the data pipeline example, what is the name of the Kafka topic where user records are published?
```

**Options**

```markdown
- A. users
- B. kafka_users
- C. postgres_users
- D. elasticsearch_users
```

<details><summary>Response:</summary>

**Answer:** A

**Explanation:**

```markdown
- A. ✅ Correct – The example mentions a topic named `users`.
- B. Not mentioned in the document.
- C. Not used – connector handles PostgreSQL, but topic name is simpler.
- D. Elasticsearch is the sink, not the topic name.
```

</details>

---

## Question 8

```markdown
What is the benefit of using Kafka Connect over writing custom ingestion code?
```

**Options**

```markdown
- A. It bypasses Kafka brokers for speed
- B. It handles integration through configuration, not code
- C. It generates Kafka topics automatically
- D. It replaces the need for schema definitions
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. Kafka brokers are always used – this is false.
- B. ✅ Correct – Kafka Connect uses config-based setup.
- C. You define topics separately – not auto-generated.
- D. Schema Registry may be used, but schemas are not "replaced."
```

</details>

---

## Question 9

```markdown
Which of the following pairs best represents a typical source-to-sink flow described in the document?
```

**Options**

```markdown
- A. MySQL → Kafka → S3
- B. MQTT → Kafka → PostgreSQL
- C. PostgreSQL → Kafka → Elasticsearch
- D. File System → Kafka → Kafka Streams
```

<details><summary>Response:</summary>

**Answer:** C

**Explanation:**

```markdown
- A. Valid flow, but not shown in this document.
- B. Would require reverse direction sink – not typical.
- C. ✅ Correct – Exact example given in the text.
- D. Kafka Streams is processing, not a sink.
```

</details>

---

## Question 10

```markdown
Which of the following best describes the Elasticsearch Sink Connector's purpose?
```

**Options**

```markdown
- A. Stream Kafka data into Amazon S3
- B. Index Kafka data for search and analytics
- C. Store Kafka messages in a relational database
- D. Generate Kafka topics dynamically
```

<details><summary>Response:</summary>

**Answer:** B

**Explanation:**

```markdown
- A. This is S3 Sink’s job.
- B. ✅ Correct – Elasticsearch indexes data for search/analytics.
- C. This is handled by the JDBC Sink.
- D. Topic creation is manual/config-based.
```

</details>

---

## Question 11

```markdown  
A media company streams live video content, which generates logs of viewer interactions (e.g., play, pause, stop) in real-time. To enhance viewer experience through personalized content and advertisements, they need to analyze these logs in real-time. The logs are stored in NoSQL databases across different geographical locations. Considering the need for low-latency analysis, which setup is most appropriate?  
```  

**Options**
```markdown  
- A. Use Kafka Connect with a custom NoSQL Source Connector for each geographical location to ingest logs into Kafka, then utilize Kafka Streams for real-time analysis and dynamic content delivery  
- B. Directly stream logs from NoSQL databases to Kafka using log-based Change Data Capture (CDC) connectors specific to each NoSQL database type, followed by processing with ksqlDB to generate viewer insights and personalized content recommendations  
- C. Configure a network of MQTT brokers to collect logs from each location, and then use an MQTT Source Connector to consolidate logs into Kafka. Apply a Kafka Streams application to analyze viewer interactions and adjust content recommendations  
- D. Implement a batch ETL process to extract logs from NoSQL databases nightly, load them into Kafka for next-day processing with Kafka Streams, and update content recommendations based on the analysis  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
CDC connectors provide the lowest latency for real-time data capture from NoSQL databases.  

- A. Custom connectors work but may have higher latency than CDC  
- B. Correct. CDC + ksqlDB provides optimal real-time processing  
- C. MQTT adds unnecessary complexity for database logs  
- D. Batch processing can't meet real-time requirements  
```  

</details>  

---  

## Question 12

```markdown  
An online retailer integrates user reviews from their website into Kafka to perform sentiment analysis and adjust product rankings accordingly. Reviews are initially posted to a MongoDB database. To ensure the analysis reflects recent feedback, which configuration ensures the most efficient and timely data flow into Kafka?  
```  

**Options**
```markdown  
- A. Configure MongoDB Source Connector to capture new and updated reviews into Kafka, then use Kafka Streams for sentiment analysis and to adjust product rankings in near-real-time  
- B. Utilize a cron job to export reviews from MongoDB to CSV files at regular intervals, then use File Source Connectors to ingest these files into Kafka, followed by processing with Kafka Streams  
- C. Deploy log-based CDC connectors to stream only the changes (new and updated reviews) from MongoDB into Kafka, leveraging ksqlDB for continuous sentiment analysis and product ranking adjustments  
- D. Directly access the MongoDB API from Kafka Streams applications to fetch new and updated reviews, perform sentiment analysis, and update product rankings without storing reviews in Kafka  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
CDC provides the most efficient capture of changes with minimal latency.  

- A. Standard source connector may capture full data unnecessarily  
- B. Batch processing introduces unacceptable delays  
- C. Correct. CDC + ksqlDB enables real-time processing  
- D. Bypasses Kafka's benefits of decoupled processing  
```  

</details>  

---  

## Question 13

```markdown  
A financial institution aims to merge transaction data from legacy systems with real-time fraud detection models running on Kafka Streams. The transaction data resides in various legacy databases and must be enriched with real-time fraud signals before being presented on a dashboard. What's the most effective architecture for this use case?  
```  

**Options**
```markdown  
- A. Use JDBC Source Connectors to ingest transaction data from legacy databases into Kafka. Enrich the data using Kafka Streams by joining it with real-time fraud detection signals. Utilize a Kafka Connect Sink Connector to publish the enriched data to the dashboard  
- B. Implement custom Kafka Producers to extract transaction data from legacy systems, enriching the data in-flight with fraud detection signals before producing it to a Kafka topic for dashboard consumption  
- C. Directly connect the legacy databases to Kafka Streams applications using custom database clients. Perform the enrichment with real-time fraud detection signals in the application, then produce the enriched data to a Kafka topic for dashboard visualization  
- D. Leverage Change Data Capture (CDC) connectors to stream transaction data from legacy databases into Kafka. Use Kafka Streams for real-time enrichment with fraud detection signals and ksqlDB to further process and prepare the data for dashboard presentation  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
CDC provides the most efficient legacy system integration.  

- A. JDBC connectors may not provide real-time updates  
- B. Custom producers increase maintenance overhead  
- C. Tight coupling reduces system flexibility  
- D. Correct. CDC + Streams + ksqlDB provides complete solution  
```  

</details>  

---

## Question 14

```markdown  
You want to use Kafka Connect to export data from a Kafka topic to a relational database. Which type of connector should you use?  
```  

**Options**
```markdown  
- A. Source Connector  
- B. Sink Connector  
- C. Transformation Connector  
- D. Import Connector  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
Sink Connectors move data from Kafka to external systems.  

- A. Source Connectors import data into Kafka  
- B. Correct. Sink Connectors handle exports  
- C. Not a valid connector type  
- D. Not a valid connector type  
```  

</details>  

---  

## Question 15

```markdown  
You need to stream data from a Twitter feed into a Kafka topic for real-time processing. Which Kafka Connect connector type is most appropriate?  
```  

**Options**
```markdown  
- A. Sink Connector  
- B. Source Connector  
- C. Transformation Connector  
- D. Export Connector  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
Source Connectors import external data into Kafka.  

- A. Sink Connectors export data from Kafka  
- B. Correct. Twitter integration requires a Source Connector  
- C. Not a valid connector type  
- D. Not a valid connector type  
```  

</details>  

---  

## Question 16

```markdown  
You are using Kafka Connect to move data from a source system into Kafka for real-time processing with Kafka Streams. After processing, the results need to be stored in HDFS for batch analysis. Which combination of connector types will you need?  
```  

**Options**
```markdown  
- A. Source Connector -> Sink Connector  
- B. Sink Connector -> Source Connector  
- C. Source Connector -> Source Connector  
- D. Sink Connector -> Sink Connector  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
The workflow requires one connector of each type.  

- A. Correct. Source for import, Sink for export  
- B. Wrong direction for both connectors  
- C. Can't export to HDFS with Source Connector  
- D. Can't import data with Sink Connector  
```  

</details>  

---  

## Question 17

```markdown  
A logistics company tracks shipping containers across the globe in real-time. This tracking information is stored in various formats across different databases. The company aims to centralize this data into Kafka for real-time visibility and to enable reactive logistics management. The IT team is proficient in SQL but new to Kafka. Which approach should they take to integrate this diverse data into Kafka efficiently?  
```  

**Options**
```markdown  
- A. Use JDBC Source Connectors to ingest container tracking data into Kafka. Transform this data into a unified format using Kafka Streams for real-time logistics management  
- B. Streamline data into Kafka using custom scripts that extract data from databases and publish to Kafka, relying on Kafka Streams for necessary transformations  
- C. Consolidate data in the RDBMS using SQL procedures to match the target schema required by Kafka, then use JDBC Source Connectors to stream this unified data into Kafka  
- D. Ingest raw data into Kafka using JDBC Source Connectors, then employ ksqlDB to perform SQL-like transformations and route the data to various microservices  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
ksqlDB leverages the team's SQL skills while providing Kafka integration.  

- A. Requires learning Kafka Streams' Java API  
- B. Custom scripts increase maintenance overhead  
- C. Adds unnecessary pre-processing complexity  
- D. Correct. Uses familiar SQL with Kafka-native tools  
```  

</details>  

---  

## Question 18

```markdown  
A retail company wishes to analyze customer transactions in real-time to personalize marketing efforts. Transaction data, including purchases and returns, is captured in a legacy system. The marketing team requires this data in a format that can be easily queried and joined with other customer data. Given the team's familiarity with SQL, what is the most effective way to achieve this?  
```  

**Options**
```markdown  
- A. Use JDBC Source Connectors to ingest transaction data into Kafka, followed by Kafka Streams for data transformation and enrichment  
- B. Directly export transaction data to CSV files, use custom producers to send these files to Kafka, and then apply Kafka Streams for transformation  
- C. Ingest transaction data into Kafka using JDBC Source Connectors and leverage ksqlDB for transforming and querying the data in a SQL-like manner  
- D. Transform data within the legacy system using SQL stored procedures, then use JDBC Source Connectors to ingest the pre-transformed data into Kafka  
```  

<details><summary>Response:</summary>  

**Answer:** C

**Explanation:**

```markdown  
ksqlDB provides SQL interface for real-time stream processing.  

- A. Requires Java/Kafka Streams expertise  
- B. CSV export introduces unnecessary steps  
- C. Correct. Matches team's SQL skills with real-time needs  
- D. Limits flexibility for future transformations  
```  

</details>  

---  

## Question 19

```markdown  
An automotive manufacturer aims to optimize its supply chain by analyzing sensor data from its manufacturing equipment in real-time. The sensor data is currently logged in a proprietary format in a traditional database. The operations team, skilled in SQL, seeks to convert this data into actionable insights. Considering their expertise and requirements, which solution would best suit their needs?  
```  

**Options**
```markdown  
- A. Utilize JDBC Source Connectors to stream sensor data into Kafka, transforming the data into a more accessible format using Kafka Streams  
- B. Export sensor data to a common format like JSON, use Kafka Connect to ingest this data, and then apply ksqlDB to analyze and visualize the data in real-time  
- C. Stream sensor data directly into Kafka using custom producers, followed by data transformation through SQL procedures embedded within the Kafka ecosystem  
- D. Ingest sensor data into Kafka using JDBC Source Connectors, then use ksqlDB to perform real-time analytics and transform the data for downstream processing  
```  

<details><summary>Response:</summary>  

**Answer:** D

**Explanation:**

```markdown  
Combines efficient ingestion with SQL-friendly analytics.  

- A. Requires Java development skills  
- B. Adds unnecessary data export step  
- C. Custom producers increase maintenance  
- D. Correct. Uses connectors + ksqlDB for full solution  
```  

</details>  

---  

## Question 20

```markdown  
A company plans to synchronize data between a PostgreSQL database and a Kafka cluster to enable real-time analytics. Which connector should be used to efficiently import data from PostgreSQL into Kafka?  
```  

**Options**
```markdown  
- A. JDBC Source Connector  
- B. S3 Sink Connector  
- C. Elasticsearch Sink Connector  
- D. HDFS Sink Connector  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
JDBC Source Connector is designed for database ingestion.  

- A. Correct. Specialized for relational database sources  
- B. Exports to S3, wrong direction  
- C. Exports to Elasticsearch  
- D. Exports to HDFS  
```  

</details>  

---  

## Question 21

```markdown  
Your organization requires real-time text search capabilities on data streamed through Kafka. Which connector best facilitates exporting data from Kafka topics into Elasticsearch to meet this requirement?  
```  

**Options**
```markdown  
- A. JDBC Sink Connector  
- B. Elasticsearch Sink Connector  
- C. MongoDB Sink Connector  
- D. Kafka Connect File Sink Connector  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
Elasticsearch Sink Connector enables search functionality.  

- A. For relational databases  
- B. Correct. Direct integration with Elasticsearch  
- C. For MongoDB, not optimized for search  
- D. Writes to filesystem  
```  

</details>  

---  

## Question 22

```markdown  
A streaming application is designed to process events in real-time and then store processed events in an Amazon S3 bucket for long-term analysis. Which connector configuration is most appropriate for this use case?  
```  

**Options**
```markdown  
- A. JDBC Source Connector  
- B. S3 Sink Connector  
- C. MQTT Source Connector  
- D. File Source Connector  
```  

<details><summary>Response:</summary>  

**Answer:** B

**Explanation:**

```markdown  
S3 Sink Connector handles Kafka-to-S3 export.  

- A. Imports from databases  
- B. Correct. Designed for S3 exports  
- C. For IoT device ingestion  
- D. Reads from filesystem  
```  

</details>  

---  

## Question 23

```markdown  
An IoT company collects temperature and humidity data from sensors deployed in various locations. The goal is to correlate this environmental data with location-specific weather forecasts retrieved from an external API. Which approach best facilitates this integration and processing within the Kafka ecosystem?  
```  

**Options**
```markdown  
- A. Ingest sensor data into a Kafka topic using MQTT connectors. Separately, use an external service to fetch weather forecasts, storing this data in a Kafka topic via the HTTP Source Connector. Utilize Kafka Streams to join sensor data with weather forecasts based on location and timestamp, outputting enriched data to another topic  
- B. Directly stream sensor data and weather forecasts into Kafka using custom Kafka Producers implemented in Python. These producers perform API calls to retrieve forecasts, merge this data with sensor readings, and produce the combined records into a single Kafka topic  
- C. Use the MQTT Source Connector to ingest sensor data into Kafka. Write a Kafka Streams application that performs REST API calls to the weather forecast service for each sensor data record processed, enriching and producing enriched records to a new topic  
- D. Implement an MQTT proxy to capture sensor data into Kafka. Concurrently, utilize Kafka Connect with the JDBC Sink Connector to store sensor data in a relational database, from which an external cron job fetches weather forecasts, merges them with sensor data, and re-ingests the enriched data back into Kafka via JDBC Source Connector  
```  

<details><summary>Response:</summary>  

**Answer:** A

**Explanation:**

```markdown  
Leverages Kafka's native capabilities for clean integration.  

- A. Correct. Uses appropriate connectors and stream processing  
- B. Custom producers bypass Kafka's integration strengths  
- C. API calls in Streams app would be inefficient  
- D. Overly complex with unnecessary steps  
```  

</details>  

---  
